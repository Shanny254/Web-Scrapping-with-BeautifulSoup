{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bad2ce6-ace8-4acc-9da9-0c789b878346",
   "metadata": {},
   "source": [
    "# **Web Scraping Using BeautifulSoup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a1afc0-13f8-44f0-aa5e-751c6b40f6b1",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a5223e-42db-4db9-a933-187817d22d05",
   "metadata": {},
   "source": [
    "I am developing a comprehensive web scraper to collect structured data efficiently.  \n",
    "\n",
    "**My scraper will:**  \n",
    "\n",
    "- Navigate multiple pages dynamically, detecting and following pagination links.  \n",
    "- Extract specific data fields, including titles, dates, and descriptions.  \n",
    "- Store data in a structured format, such as CSV or JSON.  \n",
    "- Implement exception handling and logging to manage errors and track the scraping process.  \n",
    "- Optimize performance using threading or asynchronous programming for faster data extraction.  \n",
    "\n",
    "This project will ensure scalability, reliability, and efficiency in web scraping.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcc51b0-33e0-469c-b012-c01bfc7a45cb",
   "metadata": {},
   "source": [
    "# **1. Introduction**\n",
    " ### **1.1 Project Overview**\n",
    " The goal of this project is to scrape job postings from the well-known website **career point kenya**.  Our goal is to dynamically extract job data, such as:\n",
    "\n",
    " - Job Title\n",
    " - Company Name\n",
    " - Workplace\n",
    " - Information about salaries (if available)\n",
    "   \n",
    " To facilitate analysis, the retrieved data will be saved in CSV and JSON formats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec73b1b-b0b2-48b3-8c7e-971befd91bc8",
   "metadata": {},
   "source": [
    "### **1.2 Why Scrape Career Point Kenya?**\n",
    "\n",
    "- Career Point Kenya is a leading job listing website in Kenya.\n",
    "\n",
    "- It provides a large number of job listings across different categories.\n",
    "\n",
    "- Automating data collection allows for trend analysis, job market insights, and personalized job recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595b0f41-af4c-4b11-853c-f3d4f79dc527",
   "metadata": {},
   "source": [
    "# **2. Setting Up the Environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb01c31-d22e-4204-b4ae-f945bd8aaf34",
   "metadata": {},
   "source": [
    "Before we start writing our scraper, we need to install and import necessary Python libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a86b6f9-fbb4-4c96-b7ef-1115491c205f",
   "metadata": {},
   "source": [
    "### **2.1 Installing Required Libraries**\n",
    " The Python libraries that we utilize are as follows:\n",
    "\n",
    "- requests ← To retrieve web pages, send HTTP requests.\n",
    "-  BeautifulSoup4 → Extracts and parses HTML data.\n",
    "- Pandas → Effectively stores and organizes data.\n",
    "- tqdm ← Shows progress bars for lengthy processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "624e194d-eb66-4209-8f65-07c00ee310e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\sharl\\appdata\\local\\anaconda3\\lib\\site-packages (2.32.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sharl\\appdata\\local\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sharl\\appdata\\local\\anaconda3\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sharl\\appdata\\local\\anaconda3\\lib\\site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sharl\\appdata\\local\\anaconda3\\lib\\site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\sharl\\appdata\\local\\anaconda3\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\sharl\\appdata\\local\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sharl\\appdata\\local\\anaconda3\\lib\\site-packages (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\sharl\\appdata\\local\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\sharl\\appdata\\local\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\sharl\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sharl\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sharl\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sharl\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sharl\\appdata\\local\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "!pip install beautifulsoup4\n",
    "!pip install tqdm\n",
    "!pip install pandas "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbae301c-148f-419a-9ebc-c649f11ce447",
   "metadata": {},
   "source": [
    "### **2.2 Importing Necessary Modules**\n",
    "Now, let's import the installed modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03b38bc8-8f18-47d9-bdce-605890fbbadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup  \n",
    "import pandas as pd  \n",
    "import time  # To add delays between requests\n",
    "from tqdm import tqdm \n",
    "import logging  # To handle errors and logs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb105b7-010d-4693-95d1-be8a06ce407a",
   "metadata": {},
   "source": [
    "# **3. Fetching and Parsing the Web Page**\n",
    "Web scraping begins with retrieving a webpage and parsing its content (which is the process of changing data from one format to another, usually done to make the current, often unstructured, unreadable data more comprehensible).\n",
    "\n",
    "### **3.1 Understanding HTTP Requests**\n",
    "To retrieve a web page, we send an HTTP GET request to the website's URL.\n",
    "The GET method sends the encoded user information appended to the page request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a373ca3-ff94-488b-94bd-054f9d07a870",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.careerpointkenya.co.ke/category/administration-jobs-in-kenya/\"\n",
    "request= requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "\n",
    "if request.status_code == 200:\n",
    "    print(\"Successfully fetched webpage!\")\n",
    "else:\n",
    "    print(f\"Failed to fetch webpage. Status Code: {request.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edda859-f2cb-4640-80eb-ef164e20262f",
   "metadata": {},
   "source": [
    "- If the request is successful then it means the status code is 200.\n",
    "- If the request fails we get an error code (e.g., 403, 404, 500)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0743ad0-01b7-4cb6-8d25-75a1446d4276",
   "metadata": {},
   "source": [
    "### **3.2 Extracting HTML Content**\n",
    "Once the page is fetched, we need to parse its HTML structure using BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a18783a5-4f44-4d89-b259-872a02a983c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html class=\"no-js\" itemscope=\"\" itemtype=\"https://schema.org/Blog\" lang=\"en-US\" prefix=\"og: https://ogp.me/ns#\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <meta content=\"width=device-width, initial-scale=1, minimum-scale=1\" name=\"viewport\"/>\n",
      "  <link href=\"https://www.careerpointkenya.co.ke/wp-content/cache/breeze-minification/css/breeze_796d4907fe892e5727116205da472876c5d313e78c8a0b93f30f8305d68f03acb07817aafdab1327dc3464da4438eadcf8e4cc9107b1e549c9fad2c096fc3055.css\" media=\"all\" rel=\"stylesheet\" type=\"text/css\"/>\n",
      "  <title>\n",
      "   Administration Jobs In Kenya | Career Point Kenya\n",
      "  </title>\n",
      "  <meta content=\"Have you been looking for administration jobs in Kenya? Find a variety of the current administration jobs ranging from Administrative Assistant jobs to HR Manager jobs and so on. We got you covered!\" name=\"description\">\n",
      "   <meta content=\"follow, index, max-snippet:-1, max-video-preview:-1, max-image-preview:large\" name=\"robots\">\n",
      "    <link href=\"https://www.careerpointkenya.co\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(request.text, \"html.parser\")\n",
    "print(soup.prettify()[:1000])  # Print first 1000 characters for preview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a74347-0df5-4b2a-8d5e-4e92702bdf02",
   "metadata": {},
   "source": [
    "- prettify() formats the HTML so we can inspect it easily.\n",
    "- We only print 1000 characters to avoid cluttering the terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77337426-efc2-4b73-a3d4-ad19d1416ff6",
   "metadata": {},
   "source": [
    "# **4. Navigating Through Multiple Pages**\n",
    "### **4.1 Handling Pagination**\n",
    "The majority of employment websites use pagination, which distributes jobs throughout several pages.\n",
    "\n",
    " We determine the number of pages and proceed to iterate through them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cafca24a-1250-42b6-b08e-389ee6ab260c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Count Found: 1-15 of 1,530 results\n",
      "Split Text: ['1-15 ', ' 1,530 results']\n",
      "Total Jobs: 1530\n",
      " Total Pages to Scrape: 102\n"
     ]
    }
   ],
   "source": [
    "import math  # Import math to round up page numbers\n",
    "\n",
    "# Extract job count text\n",
    "job_count_element = soup.find(\"div\", class_=\"wp-block-kadence-query-result-count\")\n",
    "\n",
    "if job_count_element:\n",
    "    job_count_text = job_count_element.text.strip()\n",
    "    print(f\"Job Count Found: {job_count_text}\")\n",
    "\n",
    "#Print the split text\n",
    "    split_text = job_count_text.split(\"of\")\n",
    "    print(f\"Split Text: {split_text}\")  # This will help us see how the text is split\n",
    "\n",
    "    try:\n",
    "        # Extract total jobs\n",
    "        total_jobs = int(split_text[-1].strip().split(\" \")[0].replace(\",\", \"\"))\n",
    "        print(f\"Total Jobs: {total_jobs}\")\n",
    "\n",
    "        # Define jobs per page\n",
    "        jobs_per_page = 15  \n",
    "\n",
    "        # Calculate total pages to scrape\n",
    "        total_pages = math.ceil(total_jobs / jobs_per_page)\n",
    "        print(f\" Total Pages to Scrape: {total_pages}\")\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\" Error extracting total jobs: {e}\")\n",
    "else:\n",
    "    print(\"Job count element not found. Check the class name again.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74651193-d95b-46d5-be0e-615f9804850b",
   "metadata": {},
   "source": [
    "### **4.2 Techniques for Multi-Page Scraping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "655568c0-129d-4472-af5c-39ca6e57d406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Title: Helpdesk/Sales Support Executive\n",
      "Company Name: Travelport\n",
      "Job Title: Helpdesk/Sales Support Executive Job Travelport\n",
      "Location: Kenya\n",
      "Salary: ,\n",
      "Job Link: https://www.careerpointkenya.co.ke/2025/03/22/helpdesk-sales-support-executive-job-travelport/\n",
      "\n",
      "Job Title: Senior Administrator – Governance and HR\n",
      "Company Name: KENET\n",
      "Job Title: Senior Administrator – Governance and HR Job KENET\n",
      "Location: Kenya\n",
      "Salary: Salary not specified\n",
      "Job Link: https://www.careerpointkenya.co.ke/2025/03/22/senior-administrator-governance-and-hr-job-kenet/\n",
      "\n",
      "Job Title: HR Officer II\n",
      "Company Name: The Kisumu National Polytechnic\n",
      "Job Title: HR Officer II Job The Kisumu National Polytechnic\n",
      "Location: Kisumu\n",
      "Salary: ,\n",
      "Job Link: https://www.careerpointkenya.co.ke/2025/03/21/hr-officer-ii-job-the-kisumu-national-polytechnic/\n",
      "\n",
      "Job Title: Assistant Manager – Data Analytics & Operations\n",
      "Company Name: CIC Insurance\n",
      "Job Title: Assistant Manager – Data Analytics & Operations Job CIC Insurance\n",
      "Location: Kenya\n",
      "Salary: Salary not specified\n",
      "Job Link: https://www.careerpointkenya.co.ke/2025/03/21/assistant-manager-data-analytics-operations-job-cic-insurance/\n",
      "\n",
      "Job Title: Office Assistant\n",
      "Company Name: Office of the Registrar of Political Parties\n",
      "Job Title: Office Assistant Job Office of the Registrar of Political Parties\n",
      "Location: Nairobi\n",
      "Salary: Salary not specified\n",
      "Job Link: https://www.careerpointkenya.co.ke/2025/03/21/office-assistant-job-office-of-the-registrar-of-political-parties/\n",
      "\n",
      "Job Title: HR Specialist\n",
      "Company Name: 4G Capital\n",
      "Job Title: HR Specialist Job 4G Capital\n",
      "Location: Kenya\n",
      "Salary: Salary not specified\n",
      "Job Link: https://www.careerpointkenya.co.ke/2025/03/21/hr-specialist-job-4g-capital/\n",
      "\n",
      "Job Title: Member, Salaries and Remuneration Commission\n",
      "Company Name: PSC\n",
      "Job Title: Member, Salaries and Remuneration Commission Job PSC\n",
      "Location: Nairobi\n",
      "Salary: 6\n",
      "Job Link: https://www.careerpointkenya.co.ke/2025/03/21/member-salaries-and-remuneration-commission-job-psc/\n",
      "\n",
      "Job Title: Support Staff\n",
      "Company Name: County Government of Nyeri\n",
      "Job Title: Support Staff Job County Government of Nyeri\n",
      "Location: Kenya\n",
      "Salary: 3\n",
      "Job Link: https://www.careerpointkenya.co.ke/2025/03/21/support-staff-job-county-government-of-nyeri/\n",
      "\n",
      "Job Title: Chief Operations Officer (COO)\n",
      "Company Name: Moi Educational Centre\n",
      "Job Title: Chief Operations Officer (COO) Job Moi Educational Centre\n",
      "Location: Kenya\n",
      "Salary: Salary not specified\n",
      "Job Link: https://www.careerpointkenya.co.ke/2025/03/21/chief-operations-officer-coo-job-moi-educational-centre/\n",
      "\n",
      "Job Title: Customer Experience Officer\n",
      "Company Name: Equity Afia\n",
      "Job Title: Customer Experience Officer Job Equity Afia\n",
      "Location: Kenya\n",
      "Salary: Salary not specified\n",
      "Job Link: https://www.careerpointkenya.co.ke/2025/03/21/customer-experience-officer-dandora-job-equity-afia/\n",
      "\n",
      "Job Title: Municipal Manager\n",
      "Company Name: Homa-Bay County Public Service Board\n",
      "Job Title: Municipal Manager Job Homa-Bay County Public Service Board\n",
      "Location: Kenya\n",
      "Salary: Salary not specified\n",
      "Job Link: https://www.careerpointkenya.co.ke/2025/03/21/municipal-manager-job-homa-bay-county-public-service-board/\n",
      "\n",
      "Job Title: Admin/Bookkeeper\n",
      "Company Name: Ideon\n",
      "Job Title: Admin/Bookkeeper Job Ideon\n",
      "Location: Nairobi\n",
      "Salary: Salary not specified\n",
      "Job Link: https://www.careerpointkenya.co.ke/2025/03/20/admin-bookkeeper-job-ideon/\n",
      "\n",
      "Job Title: Board Members\n",
      "Company Name: Badili Africa\n",
      "Job Title: Board Members Job Badili Africa\n",
      "Location: Kenya\n",
      "Salary: Salary not specified\n",
      "Job Link: https://www.careerpointkenya.co.ke/2025/03/20/board-members-job-badili-africa/\n",
      "\n",
      "Job Title: HR Associate, Organizational Development\n",
      "Company Name: Aga Khan University Hospital\n",
      "Job Title: HR Associate, Organizational Development Job Aga Khan University Hospital\n",
      "Location: Nairobi\n",
      "Salary: Salary not specified\n",
      "Job Link: https://www.careerpointkenya.co.ke/2025/03/20/hr-associate-organizational-development-job-aga-khan-university-hospital-3/\n",
      "\n",
      "Job Title: Admissions Associate\n",
      "Company Name: Zetech University\n",
      "Job Title: Admissions Associate Job Zetech University\n",
      "Location: Kenya\n",
      "Salary: Salary not specified\n",
      "Job Link: https://www.careerpointkenya.co.ke/2025/03/19/admissions-associate-march-job-zetech-university/\n",
      "\n",
      "Job Title: Relationship Manager-Business\n",
      "Company Name: Credit Bank\n",
      "Job Title: Relationship Manager-Business Job Credit Bank\n",
      "Location: Kenya\n",
      "Salary: Salary not specified\n",
      "Job Link: https://www.careerpointkenya.co.ke/2025/03/19/relationship-manager-business-job-credit-bank/\n",
      "\n",
      "Job Title: Recruitment Coordinator\n",
      "Company Name: Tony Blair Institute for Global Change\n",
      "Job Title: Recruitment Coordinator Job Tony Blair Institute for Global Change\n",
      "Location: Kenya\n",
      "Salary: Salary not specified\n",
      "Job Link: https://www.careerpointkenya.co.ke/2025/03/19/recruitment-coordinator-job-tony-blair-institute-for-global-change/\n",
      "\n",
      "Job Title: Team Manager MERLA\n",
      "Company Name: CBM\n",
      "Job Title: Team Manager MERLA Job CBM\n",
      "Location: Nairobi\n",
      "Salary: Salary not specified\n",
      "Job Link: https://www.careerpointkenya.co.ke/2025/03/19/team-manager-merla-job-cbm/\n",
      "\n",
      "Job Title: Administrative Assistant\n",
      "Company Name: Epiroc\n",
      "Job Title: Administrative Assistant Job Epiroc\n",
      "Location: Kenya\n",
      "Salary: Salary not specified\n",
      "Job Link: https://www.careerpointkenya.co.ke/2025/03/19/administrative-assistant-job-epiroc/\n",
      "\n",
      "Job Title: Manager, Planning & Strategy\n",
      "Company Name: KPPF\n",
      "Job Title: Manager, Planning & Strategy Job KPPF\n",
      "Location: Kenya\n",
      "Salary: Salary not specified\n",
      "Job Link: https://www.careerpointkenya.co.ke/2025/03/19/manager-planning-strategy-job-kppf/\n",
      "\n",
      "Job Title: Front Office Trainee\n",
      "Company Name: Kempinski\n",
      "Job Title: Front Office Trainee Job Kempinski\n",
      "Location: Kenya\n",
      "Salary: Salary not specified\n",
      "Job Link: https://www.careerpointkenya.co.ke/2025/03/18/front-office-trainee-march-job-kempinski/\n",
      "\n",
      "Job Title: Administration Officer III\n",
      "Company Name: County Government of Kericho\n",
      "Job Title: Administration Officer III Job County Government of Kericho\n",
      "Location: Kenya\n",
      "Salary: Salary not specified\n",
      "Job Link: https://www.careerpointkenya.co.ke/2025/03/18/administration-officer-iii-job-county-government-of-kericho/\n",
      "\n",
      "Job Title: Manager Legal Services\n",
      "Company Name: Office of the Controller of Budget\n",
      "Job Title: Manager Legal Services Job Office of the Controller of Budget\n",
      "Location: Nairobi\n",
      "Salary: ,\n",
      "Job Link: https://www.careerpointkenya.co.ke/2025/03/18/manager-legal-services-job-office-of-the-controller-of-budget/\n",
      "\n",
      "Job Title: Operations Associate\n",
      "Company Name: UN Women\n",
      "Job Title: Operations Associate Job UN Women\n",
      "Location: Nairobi\n",
      "Salary: Salary not specified\n",
      "Job Link: https://www.careerpointkenya.co.ke/2025/03/18/operations-associate-job-un-women/\n",
      "\n",
      "Job Title: Senior Regional Operations Analyst\n",
      "Company Name: Glovo\n",
      "Job Title: Senior Regional Operations Analyst Job Glovo\n",
      "Location: Kenya\n",
      "Salary: Salary not specified\n",
      "Job Link: https://www.careerpointkenya.co.ke/2025/03/18/senior-regional-operations-analyst-job-glovo/\n",
      "\n",
      "Job Title: Head of Marketing and Fundraising\n",
      "Company Name: Adept Systems\n",
      "Job Title: Head of Marketing and Fundraising Job Adept Systems\n",
      "Location: Kenya\n",
      "Salary: Salary not specified\n",
      "Job Link: https://www.careerpointkenya.co.ke/2025/03/17/head-of-marketing-and-fundraising-job-adept-systems/\n",
      "\n",
      "Job Title: Estate Manager\n",
      "Company Name: Vipingo Development\n",
      "Job Title: Estate Manager Job Vipingo Development\n",
      "Location: Kenya\n",
      "Salary: Salary not specified\n",
      "Job Link: https://www.careerpointkenya.co.ke/2025/03/17/estate-manager-job-vipingo-development/\n",
      "\n",
      "Job Title: Accountant Intern/Receptionist\n",
      "Company Name: Noble Veterinary Surgeons\n",
      "Job Title: Accountant Intern/Receptionist Job Noble Veterinary Surgeons\n",
      "Location: Kenya\n",
      "Salary: Salary not specified\n",
      "Job Link: https://www.careerpointkenya.co.ke/2025/03/17/accountant-intern-receptionist-job-noble-veterinary-surgeons/\n",
      "\n",
      "Job Title: Administrative Assistant\n",
      "Company Name: Kenindia Assurance\n",
      "Job Title: Administrative Assistant Job Kenindia Assurance\n",
      "Location: Kenya\n",
      "Salary: Salary not specified\n",
      "Job Link: https://www.careerpointkenya.co.ke/2025/03/17/administrative-assistant-job-kenindia-assurance/\n",
      "\n",
      "Job Title: Sales Administrator\n",
      "Company Name: Talent Nexus\n",
      "Job Title: Sales Administrator Job Talent Nexus\n",
      "Location: Kenya\n",
      "Salary: Salary not specified\n",
      "Job Link: https://www.careerpointkenya.co.ke/2025/03/17/sales-administrator-job-talent-nexus/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Define job listing page URL\n",
    "base_url = \"https://www.careerpointkenya.co.ke/category/administration-jobs-in-kenya/?pg=\"\n",
    "\n",
    "# Headers to mimic a browser request\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# List of possible salary keywords\n",
    "salary_keywords = [\"salary\", \"remuneration\", \"competitive pay\", \"negotiable\"]\n",
    "location_keywords = [\"Nairobi\", \"Mombasa\", \"Kisumu\", \"Kenya\"]  # Add more cities\n",
    "\n",
    "num_pages = 2  # Number of pages to scrape\n",
    "\n",
    "# Loop through job listing pages\n",
    "for page in range(1, num_pages + 1):\n",
    "    response = requests.get(base_url + str(page), headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Extract job links\n",
    "    jobs = soup.find_all(\"a\", class_=\"kb-section-link-overlay\")\n",
    "\n",
    "    for job in jobs:\n",
    "        job_link = job[\"href\"]\n",
    "\n",
    "        # Visit each job detail page\n",
    "        job_response = requests.get(job_link, headers=headers)\n",
    "        job_soup = BeautifulSoup(job_response.text, \"html.parser\")\n",
    "\n",
    "       # Extract job title\n",
    "        job_title_elem = job_soup.find(\"h1\", class_=\"wp-block-kadence-advancedheading\")\n",
    "        job_title = job_title_elem.text.strip() if job_title_elem else \"Title not found\"\n",
    "\n",
    "        # Extract company name correctly\n",
    "        if \" Job \" in job_title:  \n",
    "            job_title_part, company_name = job_title.split(\" Job \", 1)\n",
    "        else:\n",
    "            job_title_part = job_title\n",
    "            company_name = \"Not found\"\n",
    "        \n",
    "        print(f\"Job Title: {job_title_part}\")  # Corrected job title\n",
    "        print(f\"Company Name: {company_name}\")  # Corrected company name\n",
    "\n",
    "\n",
    "        # Extract company name\n",
    "        company_name = job_title.split()[-1] if job_title != \"Title not found\" else \"Not found\"\n",
    "\n",
    "        # Extract job description text\n",
    "        job_desc = job_soup.get_text().lower()\n",
    "\n",
    "        # Check for location in job description\n",
    "        location_found = \"Location not specified\"\n",
    "        for loc in location_keywords:\n",
    "            if loc.lower() in job_desc:\n",
    "                location_found = loc\n",
    "                break\n",
    "\n",
    "        # Check for salary information\n",
    "        salary_found = \"Salary not specified\"\n",
    "        for keyword in salary_keywords:\n",
    "            match = re.search(rf\"\\b{keyword}\\b.*?:?\\s*([\\d,]+|\\b(negotiable|competitive pay)\\b)\", job_desc)\n",
    "            if match:\n",
    "                salary_found = match.group(1) if match.group(1) else \"Negotiable\"\n",
    "                break\n",
    "\n",
    "        print(f\"Job Title: {job_title}\")\n",
    "        print(f\"Location: {location_found}\")\n",
    "        print(f\"Salary: {salary_found}\")\n",
    "        print(f\"Job Link: {job_link}\\n\")\n",
    "\n",
    "        time.sleep(1)  # Pause to avoid overloading the server\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67aeca90-6252-4374-b619-0210c2a0568f",
   "metadata": {},
   "source": [
    "# **5. Data Storage**\n",
    "### **5.1 Saving to CSV**\n",
    "The CSV (Comma-Separated Values) format is useful for spreadsheet applications like Excel, Pandas and Google Sheets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "22b518e4-876d-499a-b35a-4dfc52d5c830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to scraped_jobs.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "scraped_jobs = [\n",
    "    {\"Job Title\": \"HR Associate, Organizational Development Job\", \"Company Name\": \"Aga Khan University Hospital\", \"Location\": \"Nairobi\", \"Salary\": \"Salary not specified\", \"Job Link\": \"https://www.careerpointkenya.co.ke/2025/03/20/hr-associate-organizational-development-job-aga-khan-university-hospital-3/\"},\n",
    "    {\"Job Title\": \"Board Members Job\", \"Company Name\": \"Badili Africa\", \"Location\": \"Kenya\", \"Salary\": \"Salary not specified\", \"Job Link\": \"https://www.careerpointkenya.co.ke/2025/03/20/board-members-job-badili-africa/\"},\n",
    "    {\"Job Title\": \"Admissions Associate Job\", \"Company Name\": \"Zetech University\", \"Location\": \"Kenya\", \"Salary\": \"Salary not specified\", \"Job Link\": \"https://www.careerpointkenya.co.ke/2025/03/19/admissions-associate-march-job-zetech-university/\"}\n",
    "]\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(scraped_jobs)\n",
    "\n",
    "# Save to CSV\n",
    "csv_filename = \"scraped_jobs.csv\"\n",
    "df.to_csv(csv_filename, index=False)\n",
    "print(f\"Data successfully saved to {csv_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f6d746-14b8-4ef9-ae9f-227f3b919421",
   "metadata": {},
   "source": [
    "### **5.2 Saving to JSON**\n",
    "JSON files are Useful when it comes to web applications and APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2c5353ee-44db-4557-be50-f3a52d9aa67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to scraped_jobs.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Save to JSON\n",
    "json_filename = \"scraped_jobs.json\"\n",
    "with open(json_filename, \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(scraped_jobs, json_file, indent=4)\n",
    "print(f\"Data successfully saved to {json_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eff865-31d5-45b9-9c1c-c978a3b07e67",
   "metadata": {},
   "source": [
    "# **6. Error Handling and Logging**\n",
    "The following common faults will be handled using **try-except blocks**: \n",
    "- Connection errors (such as internet problems)\n",
    "- HTTP errors (such as page not found, server problems)\n",
    "- Parsing errors (such as missing fields). handy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fd94ee-740a-4bb1-8ae6-51f2807513ab",
   "metadata": {},
   "source": [
    "**Logging** is useful for tracking:\n",
    "\n",
    "- When someone make a request for something\n",
    "- The errors that were encountered throughout the scrapping processs\n",
    "- Count of jobs scrapped accordingly, depending on user needs\n",
    "\n",
    " This is where Python's **logging** package comes in handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1275ba12-d720-494b-9fc3-79caa57dd52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed. Check 'scraper.log' for details.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename=\"scraper.log\", level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# Base URL\n",
    "BASE_URL = \"https://www.careerpointkenya.co.ke/page/\"\n",
    "\n",
    "# List to store job details\n",
    "scraped_jobs = []\n",
    "\n",
    "# Function to scrape job listings\n",
    "def scrape_jobs():\n",
    "    for page in range(1, 103):  # Scraping all 102 pages\n",
    "        url = f\"{BASE_URL}{page}/\"\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status()  # Raise HTTP errors\n",
    "\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            job_posts = soup.find_all(\"div\", class_=\"job-listing\")  # Adjust this selector as needed\n",
    "\n",
    "            for job in job_posts:\n",
    "                try:\n",
    "                    title = job.find(\"h2\").text.strip()\n",
    "                    link = job.find(\"a\")[\"href\"] if job.find(\"a\") else \"No link\"\n",
    "\n",
    "                    scraped_jobs.append({\n",
    "                        \"Job Title\": title,\n",
    "                        \"Job Link\": link\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Skipping job due to parsing error: {e}\")\n",
    "\n",
    "            time.sleep(1)  # Prevents overloading the server\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"Failed to scrape {url}: {e}\")\n",
    "\n",
    "    logging.info(f\"Scraping completed. Total jobs scraped: {len(scraped_jobs)}\")\n",
    "\n",
    "# Run the scraper\n",
    "scrape_jobs()\n",
    "\n",
    "# Save data to CSV & JSON\n",
    "df = pd.DataFrame(scraped_jobs)\n",
    "df.to_csv(\"scraped_jobs.csv\", index=False)\n",
    "df.to_json(\"scraped_jobs.json\", orient=\"records\", indent=4)\n",
    "\n",
    "print(\"Scraping completed. Check 'scraper.log' for details.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c8b74d-6682-4fc1-bf97-4b15d2b4c2e3",
   "metadata": {},
   "source": [
    "# **7.Performance Optimization**\n",
    "We improve scraping efficiency using **multi-threading** and **asynchronous requests**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c889e29a-eee1-4a70-8c83-e48842ad44c8",
   "metadata": {},
   "source": [
    "**Multi-threading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a2b083d4-fdfa-4341-a58d-e6d2954cc306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-threaded Scraping Complete! Data saved.\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import logging\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename=\"thread_scraper.log\", level=logging.ERROR, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "BASE_URL = \"https://www.careerpointkenya.co.ke/page/\"\n",
    "scraped_jobs = []\n",
    "\n",
    "def fetch_page(page):\n",
    "    \"\"\"Fetch a page using requests.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{BASE_URL}{page}/\", timeout=10)\n",
    "        return response.text if response.status_code == 200 else None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fetching page {page}: {e}\")\n",
    "        return None\n",
    "\n",
    "def parse_jobs(html):\n",
    "    \"\"\"Extract job details from page HTML.\"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    jobs = soup.find_all(\"div\", class_=\"job-listing\")  # Adjust selector if needed\n",
    "    return [\n",
    "        {\n",
    "            \"Job Title\": job.find(\"h2\").text.strip(),\n",
    "            \"Company Name\": job.find(\"p\", class_=\"company-name\").text.strip() if job.find(\"p\", class_=\"company-name\") else \"Not specified\",\n",
    "            \"Location\": job.find(\"p\", class_=\"job-location\").text.strip() if job.find(\"p\", class_=\"job-location\") else \"Not specified\",\n",
    "            \"Salary\": job.find(\"p\", class_=\"salary\").text.strip() if job.find(\"p\", class_=\"salary\") else \"Salary not specified\",\n",
    "            \"Job Link\": job.find(\"a\")[\"href\"] if job.find(\"a\") else \"No link available\"\n",
    "        }\n",
    "        for job in jobs\n",
    "    ]\n",
    "\n",
    "def scrape_jobs_multithreaded(pages=10, max_threads=5):\n",
    "    \"\"\"Scrape jobs using multi-threading.\"\"\"\n",
    "    with ThreadPoolExecutor(max_threads) as executor:\n",
    "        responses = list(executor.map(fetch_page, range(1, pages + 1)))\n",
    "\n",
    "    # Parse HTML pages in parallel\n",
    "    global scraped_jobs\n",
    "    with ThreadPoolExecutor(max_threads) as executor:\n",
    "        results = list(executor.map(parse_jobs, [html for html in responses if html]))\n",
    "\n",
    "    # Flatten results\n",
    "    scraped_jobs = [job for sublist in results for job in sublist]\n",
    "\n",
    "    # Save data\n",
    "    pd.DataFrame(scraped_jobs).to_csv(\"thread_scraped_jobs.csv\", index=False)\n",
    "    with open(\"thread_scraped_jobs.json\", \"w\") as f:\n",
    "        json.dump(scraped_jobs, f, indent=4)\n",
    "\n",
    "# Run scraper\n",
    "scrape_jobs_multithreaded(pages=10, max_threads=5)\n",
    "print(f\"Multi-threaded Scraping Complete! Data saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79577d6f-91cf-4c37-8b5b-811d2f035147",
   "metadata": {},
   "source": [
    "**Asynchronous requests**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "79925ab8-c991-47e4-964e-7f455391b12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Async Scraping Complete! Data saved.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import pandas as pd\n",
    "import json\n",
    "import logging\n",
    "from bs4 import BeautifulSoup\n",
    "import nest_asyncio  # Fixes event loop issues in Jupyter\n",
    "\n",
    "# Apply fix for Jupyter Notebook\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename=\"async_scraper.log\", level=logging.ERROR, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "BASE_URL = \"https://www.careerpointkenya.co.ke/page/\"\n",
    "scraped_jobs = []\n",
    "\n",
    "async def fetch_page(session, page):\n",
    "    \"\"\"Asynchronously fetch a page.\"\"\"\n",
    "    try:\n",
    "        async with session.get(f\"{BASE_URL}{page}/\", timeout=10) as response:\n",
    "            return await response.text() if response.status == 200 else None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fetching page {page}: {e}\")\n",
    "        return None\n",
    "\n",
    "def parse_jobs(html):\n",
    "    \"\"\"Parse job data from HTML.\"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    jobs = soup.find_all(\"div\", class_=\"job-listing\")  # Adjust selector if needed\n",
    "    return [\n",
    "        {\n",
    "            \"Job Title\": job.find(\"h2\").text.strip(),\n",
    "            \"Company Name\": job.find(\"p\", class_=\"company-name\").text.strip() if job.find(\"p\", class_=\"company-name\") else \"Not specified\",\n",
    "            \"Location\": job.find(\"p\", class_=\"job-location\").text.strip() if job.find(\"p\", class_=\"job-location\") else \"Not specified\",\n",
    "            \"Salary\": job.find(\"p\", class_=\"salary\").text.strip() if job.find(\"p\", class_=\"salary\") else \"Salary not specified\",\n",
    "            \"Job Link\": job.find(\"a\")[\"href\"] if job.find(\"a\") else \"No link available\"\n",
    "        }\n",
    "        for job in jobs\n",
    "    ]\n",
    "\n",
    "async def scrape_jobs_async(pages=10):\n",
    "    \"\"\"Scrape jobs using async HTTP requests.\"\"\"\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [fetch_page(session, page) for page in range(1, pages + 1)]\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "\n",
    "    # Process responses\n",
    "    global scraped_jobs\n",
    "    scraped_jobs = [job for html in responses if html for job in parse_jobs(html)]\n",
    "\n",
    "    # Save data\n",
    "    pd.DataFrame(scraped_jobs).to_csv(\"async_scraped_jobs.csv\", index=False)\n",
    "    with open(\"async_scraped_jobs.json\", \"w\") as f:\n",
    "        json.dump(scraped_jobs, f, indent=4)\n",
    "\n",
    "# Run scraper in Jupyter\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(scrape_jobs_async(pages=10))\n",
    "\n",
    "print(\"Async Scraping Complete! Data saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffee140b-8c93-41c9-9130-fec35cabf4c1",
   "metadata": {},
   "source": [
    "# **8. Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d897e8-3f10-4302-b2fe-4ed84540640b",
   "metadata": {},
   "source": [
    "This web scraper successfully collects job listings from multiple pages, extracts structured data, and stores it in CSV and JSON formats.  It has logging, strong error handling, and asynchronous programming for performance optimization.  Although the script satisfies all requirements, there is room for development in terms of compliance, scalability, and data extraction accuracy.  The project shows off Python's web scraping capabilities and the value of thoughtful design in striking a balance between speed and tolerance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4aeaaf-cf42-413d-b41c-9519f180ffe8",
   "metadata": {},
   "source": [
    "### **Future Improvements**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811478c1-afd6-48ae-a198-8701cb0547b6",
   "metadata": {},
   "source": [
    "**1. Advanced Data Extraction:** \n",
    "\n",
    "- To more effectively extract location and wage information from unstructured text, apply natural language processing (NLP).\n",
    "- Add new fields, such as the date of the job posting or the credentials needed.\n",
    "  \n",
    "**2.Scalability:** \n",
    "- To manage more extensive scraping over several categories or websites, put in place a task queue (for example, with Celery or AIjobs).\n",
    "- Install a database (such as PostgreSQL or SQLite) for querying and persistent storing.\n",
    "  \n",
    "**3.Robustness:**\n",
    "- To prevent detection and blocking, incorporate user-agent and proxy rotation.\n",
    "- For rate-limited queries, use more complex retry logic with exponential backoff.\n",
    "  \n",
    "**4.Performance:**\n",
    "- For webpages that need JavaScript rendering, use a headless browser (like Playwright).\n",
    "- Instead of storing all data in memory, stream data to files to maximize memory use.\n",
    "  \n",
    "**5.Compliance:** \n",
    "- To guarantee ethical scraping, include a check for the website's **robots.txt** and terms of service.\n",
    "- Provide a way to halt or suspend scraping in the event that the server returns a 429 (Too Many Requests) status.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a16324-d776-444d-8038-4cef3cafd333",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
